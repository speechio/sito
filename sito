#!/usr/bin/env python3
# coding = utf8
# Copyright 2021  Author:  Jiayu DU  (SpeechIO)

import sys, os
import uuid
import argparse
import json
import time
from collections import deque
import ffmpeg
import torch
import torch.nn.functional as F
import torchaudio
torch.set_num_threads(1)

FFMPEG_WAV_HEADER_BYTES = 78
FFMPEG_WAV_SAMPLE_BYTES = 2
SITO_SAMPLE_RATE = 16000
SITO_ROOT = '/Users/jerry/work/git/sito'
SAFE = os.path.join(SITO_ROOT, '.safe')


def speech_recognition_aispeech(session_key, audio_data):
  import subprocess
  # with open(os.path.join(SAFE, 'aispeech', 'PRODUCT_ID'), 'r') as f:
  #   PRODUCT_ID = f.readline().strip()

  # with open(os.path.join(SAFE, 'aispeech', 'API_KEY'), 'r') as f:
  #   API_KEY= f.readline().strip()
  
  # SERVICE_URL = 'https://lasr.duiopen.com/lasr-sentence-api/v2/sentence'
  # URL=SERVICE_URL + '?' + 'productId=' + PRODUCT_ID + '&' + 'apiKey=' + API_KEY
  # PARAMS='\'{"request_id":"", "audio": {"audio_type": "wav","sample_rate": 16000,"channel": 1,"sample_bytes": 2}, "asr":{"use_vad":true, "use_itn":true, "use_puctuation":true}}\''

  # cmd='curl -X POST -s -H "Content-Type: multipart/form-data"' + ' -F params=' + PARAMS + ' -F file=@' + audio + ' "' + URL + '"'  # TODO: here audio is filename, modify this to raw binary audio data
  # r = subprocess.getoutput(cmd)
  # print(F'key={session_key}', F'service=AISPEECH', r)

  # text = ''
  # if r:
  #   for sentence in json.loads(r)['data']['result']:
  #     text += sentence['onebest']
  return ''


def speech_recognition_aliyun(session_key, audio_data):
  # DOC: https://help.aliyun.com/document_detail/72153.html
  # INSTALL: pip3 install aliyun-python-sdk-core==2.13.3
  import http.client
  from aliyunsdkcore.client import AcsClient
  from aliyunsdkcore.request import CommonRequest

  with open(os.path.join(SAFE, 'aliyun', 'ACCESS_KEY_ID'), 'r') as f:
      ACCESS_KEY_ID = f.readline().strip()

  with open(os.path.join(SAFE, 'aliyun', 'ACCESS_KEY_SECRET'), 'r')  as f:
      ACCESS_KEY_SECRET = f.readline().strip()

  with open(os.path.join(SAFE, 'aliyun', 'APPKEY'), 'r') as f:
      APPKEY = f.readline().strip()

  # Get API certificate token
  client = AcsClient(ACCESS_KEY_ID, ACCESS_KEY_SECRET, "cn-shanghai")
  request = CommonRequest()
  request.set_method('POST')
  request.set_domain('nls-meta.cn-shanghai.aliyuncs.com')
  request.set_version('2019-02-28')
  request.set_action_name('CreateToken')
  response = json.loads(client.do_action_with_exception(request))
  token = response['Token']['Id']
  #print(token, file=sys.stderr)

  FORMAT = 'wav'
  SAMPLE_RATE = 16000
  enable_punctuation_prediction = 'true'
  enable_inverse_text_normalization = 'true'
  enable_voice_detection = 'false'

  # Setup request URL
  URL = ''.join(['http://nls-gateway.cn-shanghai.aliyuncs.com/stream/v1/asr', '?appkey=' + APPKEY, '&format=' + FORMAT, '&sample_rate=' + str(SAMPLE_RATE)])
  URL += '&enable_punctuation_prediction=' + enable_punctuation_prediction
  URL += '&enable_inverse_text_normalization=' + enable_inverse_text_normalization
  URL += '&enable_voice_detection=' + enable_voice_detection
  #print(URL)

  # Setup request header
  header = {
    'X-NLS-Token': token,
    'Content-type': 'application/octet-stream',
    'Content-Length': len(audio_data)
  }

  # send request & get response
  host = 'nls-gateway.cn-shanghai.aliyuncs.com'
  c = http.client.HTTPConnection(host)
  c.request(method='POST', url=URL, body=audio_data, headers=header)
  r = json.loads(c.getresponse().read())

  # logging and return result
  print(F'key={session_key}', F'service=ALIYUN', r, file=sys.stderr)
  if r['status'] == 20000000 and r['message'] == 'SUCCESS':
    return r['result']
  else:
    return ''


def speech_recognition_baidu(session_key, audio_data):
  # DOC: https://github.com/Baidu-AIP/speech-demo/rest-api-asr/python/asr_raw.py
  # INSTALL: no extra dependency
  from urllib.request import urlopen
  from urllib.request import Request
  from urllib.error import URLError
  from urllib.parse import urlencode

  # authentication
  with open(os.path.join(SAFE, 'baidu_pro', 'API_KEY'), 'r') as f:
      API_KEY = f.readline().strip()
  with open(os.path.join(SAFE, 'baidu_pro', 'SECRET_KEY'), 'r') as f:
      SECRET_KEY = f.readline().strip()

  # get API certificate token
  TOKEN_URL = 'http://openapi.baidu.com/oauth/2.0/token'
  token_params = {'grant_type': 'client_credentials', 'client_id': API_KEY, 'client_secret': SECRET_KEY}
  req = Request(TOKEN_URL, urlencode(token_params).encode('utf-8'))
  r = json.loads(urlopen(req).read().decode())
  #print(r, file=sys.stderr)
  token = r['access_token']

  ## 普通版
  ## 1537 表示识别普通话，使用输入法模型。
  ## 1536表示识别普通话，使用搜索模型。
  ## 根据文档填写PID，选择语言及识别模型
  #DEV_PID = 1537;
  #ASR_URL = 'http://vop.baidu.com/server_api'
  #SCOPE = 'audio_voice_assistant_get'  # 有此scope表示有asr能力，没有请在网页里勾选，非常旧的应用可能没有

  #测试自训练平台需要打开以下信息， 自训练平台模型上线后，您会看见 第二步：“”获取专属模型参数pid:8001，modelid:1234”，按照这个信息获取 dev_pid=8001，lm_id=1234
  # DEV_PID = 8001 ;
  # LM_ID = 1234 ;

  # 极速版 打开注释的话请填写自己申请的appkey appSecret ，并在网页中开通极速版（开通后可能会收费）
  DEV_PID = 80001
  ASR_URL = 'http://vop.baidu.com/pro_api'
  #SCOPE = 'brain_enhanced_asr'  # 有此scope表示有asr能力，没有请在网页里勾选，非常旧的应用可能没有

  CUID = '123456PYTHON'
  FORMAT = 'wav'
  SAMPLE_RATE = 16000

  # send request & get response
  params = {'cuid': CUID, 'token': token, 'dev_pid': DEV_PID}
  #测试自训练平台需要打开以下信息
  #params = {'cuid': CUID, 'token': token, 'dev_pid': DEV_PID, 'lm_id' : LM_ID}
  assert(len(audio_data) != 0)
  header = {
      'Content-Type': 'audio/' + FORMAT + '; rate=' + str(SAMPLE_RATE),
      'Content-Length': len(audio_data)
  }
  req = Request(ASR_URL + "?" + urlencode(params), audio_data, header)
  r = json.loads(urlopen(req).read())

  # logging and return result
  print(F'key={session_key}', F'service=BAIDU_PRO', r, file=sys.stderr)
  if r['err_no'] == 0:
    return r['result'][0].strip() # use 1-best result
  else:
    return ''


def speech_recognition_google(session_key, audio_data):
  # DOC: https://cloud.google.com/speech-to-text/
  # DEMO: https://github.com/googleapis/python-speech/blob/master/samples/snippets/transcribe.py
  # INSTALL: pip3 install --upgrade google-cloud-speech
  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(SAFE, 'google', 'TIOBE.json')
  from google.cloud import speech

  config = speech.RecognitionConfig(
      language_code="zh",
      sample_rate_hertz=16000,
      encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
  )
  client = speech.SpeechClient()
  r = client.recognize(config=config, audio=speech.RecognitionAudio(content=audio_data))
  return ''.join([result.alternatives[0].transcript for result in r.results])

def speech_recognition_microsoft(session_key, audio_data):
  # DOC: https://docs.azure.cn/zh-cn/cognitive-services/speech-service/rest-speech-to-text
  # INSTLL: pip3 install requests
  import requests
  LANG='zh-CN'
  with open(os.path.join(SAFE, 'microsoft', 'SUBSCRIPTION_KEY'), 'r') as f:
    SUBSCRIPTION_KEY = f.readline().strip()

  url='https://chinaeast2.stt.speech.azure.cn/speech/recognition/conversation/cognitiveservices/v1?language=' + LANG

  headers = {
      'Accept': 'application/json;text/xml',
      'Content-Type': 'audio/wav;codecs="audio/pcm";samplerate=16000',
      'Ocp-Apim-Subscription-Key': SUBSCRIPTION_KEY,
      'format': 'detailed'
  }

  r = requests.post(url, data=audio_data, headers=headers)
  print(F'key={session_key}', F'service=MICROSOFT', r.text, file=sys.stderr)
  if r:
    return json.loads(r.text)['DisplayText']
  else:
    return ''

def speech_recognition_sogou(session_key, audio_data):
  # DOC: https://ai.sogou.com/doc/?url=/docs/content/asr/recognize/
  # INSTALL: no extra dependency
  import requests
  import base64
  import subprocess

  FORMAT='LINEAR16'
  SAMPLE_RATE=16000
  LANG='zh-cmn-Hans-CN'

  with open(os.path.join(SAFE, 'sogou', 'APP_ID'), 'r') as f:
      APP_ID = f.readline().strip()

  with open(os.path.join(SAFE, 'sogou', 'APP_KEY'), 'r') as f:
      APP_KEY = f.readline().strip()

  EXPIRE = '60s'
  REQ=F'{{"appid": "{APP_ID}", "appkey": "{APP_KEY}", "exp": "{EXPIRE}"}}'
  cmd = F'curl -s -X POST -H "Content-Type: application/json" --data \'{REQ}\' https://api.zhiyin.sogou.com/apis/auth/v1/create_token '
  #print(cmd, file=sys.stderr)
  token_string = subprocess.getoutput(cmd)
  #print(token_string, file=sys.stderr)
  TOKEN = json.loads(token_string)['token']

  data = {
      'config': {
          'encoding': FORMAT,
          'sample_rate_hertz': SAMPLE_RATE,
          'language_code': LANG
      },
      'audio': {
          'content': base64.b64encode(audio_data).decode('ascii')
      }
  }
  headers = {
      'Content-Type': 'application/json',
      'Appid': APP_ID,
      'Authorization': 'Bearer ' + TOKEN
  }

  r = requests.post('https://api.zhiyin.sogou.com/apis/asr/v1/recognize', data=json.dumps(data), headers=headers)
  print(F'key={session_key}', F'service=SOGOU', r.text)
  text = ''
  if r:
    for sentence in json.loads(r.text)['results']:
        text += sentence['alternatives'][0]['transcript']
  return text


def speech_recognition_tencent(session_key, audio_data):
  # DEMO: https://cloud.tencent.com/document/product/1093/35734
  # INSTALL: pip3 install tencentcloud-sdk-python
  from tencentcloud.common import credential
  from tencentcloud.common.profile.client_profile import ClientProfile
  from tencentcloud.common.profile.http_profile import HttpProfile
  from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException
  from tencentcloud.asr.v20190614 import asr_client, models
  import base64

  with open(os.path.join(SAFE, 'tencent', 'SECRET_ID'), 'r') as f:
      SECRET_ID = f.readline().strip()

  with open(os.path.join(SAFE, 'tencent', 'SECRET_KEY'), 'r') as f:
      SECRET_KEY = f.readline().strip()

  audio_len = len(audio_data)
  audio_data_base64 = base64.b64encode(audio_data).decode('utf-8')

  cred = credential.Credential(SECRET_ID, SECRET_KEY)
  httpProfile = HttpProfile()
  httpProfile.endpoint = "asr.tencentcloudapi.com"
  clientProfile = ClientProfile()
  clientProfile.httpProfile = httpProfile
  client = asr_client.AsrClient(cred, "ap-shanghai", clientProfile)

  req = models.SentenceRecognitionRequest()
  params = {"ProjectId":0,"SubServiceType":2,"EngSerViceType":"16k","SourceType":1,"Url":"","VoiceFormat":"wav","UsrAudioKey":"session-123", "Data":audio_data_base64, "DataLen":audio_len}
  req._deserialize(params)

  r = client.SentenceRecognition(req)
  print(F'key={session_key}', F'service=TENCENT', r.to_json_string(), file=sys.stderr)
  if r:
    return json.loads(r.to_json_string())['Result']
  else:
    return ''


def speech_recognition_yitu(session_key, audio_data):
  # DOC: https://speech.yitutech.com/devdoc/shortaudio
  # INSTALL: pip3 install requests
  import hmac, base64, hashlib
  import requests

  ASR_URL = 'http://asr-prod.yitutech.com/v2/asr'
  AUDIO_AUE = 'pcm'  # "pcm" for .wav files
  SCENE = 0   # default
  LANG = 1    # default

  with open(os.path.join(SAFE, 'yitu', 'DEV_ID'), 'r') as f:
      DEV_ID = f.readline().strip()

  with open(os.path.join(SAFE, 'yitu', 'DEV_KEY'), 'r') as f:
      DEV_KEY = f.readline().strip()

  # request header
  time_stamp = str(int(time.time()))
  sign = hmac.new(DEV_KEY.encode(), (str(DEV_ID) + time_stamp).encode(), digestmod=hashlib.sha256).hexdigest()
  headers = {'x-dev-id': str(DEV_ID), 'x-request-send-timestamp': time_stamp, 'x-signature': sign}

  # request body
  useCustomWordsIds = [] # hints
  audio_data_base64 = base64.b64encode(audio_data).decode('utf-8')
  body = {'audioBase64': audio_data_base64, 'lang': LANG, 'scene': SCENE, 'aue': AUDIO_AUE, 'useCustomWordsIds': useCustomWordsIds}

  # send request & get result
  r = requests.post(ASR_URL, json=body, headers=headers)

  # logging and return result
  print(F'key={session_key}', F'service=YITU', r.json(), file=sys.stderr)
  if r.status_code == 200:
    return r.json()['resultText']
  else:
    return ''


def sito(service, session_key, audio_data):
  speech_recognition = {
    'AISPEECH': speech_recognition_aispeech,
    'ALIYUN': speech_recognition_aliyun,
    'BAIDU_PRO': speech_recognition_baidu,
    'GOOGLE': speech_recognition_google,
    'MICROSOFT': speech_recognition_microsoft,
    'SOGOU': speech_recognition_sogou,
    'TENCENT': speech_recognition_tencent,
    'YITU': speech_recognition_yitu,
  }
  return speech_recognition[service](session_key, audio_data)


def read_audio_and_convert_to_bytes(in_filename, **input_kwargs):
  try:
    out, err = (ffmpeg
      .input(in_filename, **input_kwargs)
      .output('-', format='wav', acodec='pcm_s16le', ac=1, ar='16k')
      .overwrite_output()
      .run(capture_stdout=True, capture_stderr=True)
    )
  except ffmpeg.Error as e:
    print(e.stderr, file=sys.stderr)
    sys.exit(1)
  return out

def read_audio_and_convert_to_tensor(path):
    wav, sample_rate = torchaudio.load(path)

    if wav.size(0) > 1:
        wav = wav.mean(dim=0, keepdim=True)

    if sample_rate != SITO_SAMPLE_RATE:
        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=SITO_SAMPLE_RATE)
        wav = transform(wav)
        sample_rate = SITO_SAMPLE_RATE

    assert sample_rate == SITO_SAMPLE_RATE
    return wav.squeeze(0)

def load_jit_vad_model(model_path: str, device=torch.device('cpu')):
    torch.set_grad_enabled(False)
    model = torch.jit.load(model_path, map_location=device)
    model.eval()
    return model

def vad_model_inference(model, audio_window: torch.Tensor, sample_rate: int = 16000):
    assert(sample_rate == SITO_SAMPLE_RATE)
    with torch.no_grad():
        return model(audio_window)

def get_speech_segments(
        model,
        audio_tensor: torch.Tensor,
        sample_rate: int = 16000,
        window_size: int = 3200,
        window_shift: int = 1600,
        smooth_length: int = 4,
        sil2speech: float = 0.25,
        speech2sil: float = 0.07,
        batch_size: int = 10):
    X = []
    Y = []
    for pos in range(0, len(audio_tensor), window_shift):
        window = audio_tensor[pos : pos + window_size]
        if len(window) < window_size:
            window = F.pad(window, (0, window_size - len(window)))
        X.append(window.unsqueeze(0))
        # process full batches & last residual batch
        if len(X) >= batch_size or (pos + window_shift) > len(audio_tensor):
            Y.append(vad_model_inference(model, torch.cat(X, dim=0), sample_rate))
            X = []

    buffer = deque(maxlen=smooth_length)  # maxlen reached => first element dropped
    triggered = False
    segments = []
    b,e = 0,0
    for k, prob in enumerate(torch.cat(Y, dim=0)[:, 1]):
        window_center = k*window_shift + 0.5*window_size
        print(f'{k}, {window_center/sample_rate:.2f}, {prob:.2f}', file=sys.stderr)
        buffer.append(prob)
        smoothed_prob = sum(buffer) / len(buffer)
        if not triggered and smoothed_prob >= sil2speech:
            triggered = True
            b = window_shift * max(0, k - smooth_length)
        elif triggered and smoothed_prob <= speech2sil:
            e = window_shift * k
            segments.append((b,e))
            b,e = 0,0
            triggered = False

    if b != 0:
        e = len(audio_tensor)
        segments.append((b,e))
    return segments


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--service', type=str, choices = ['AISPEECH', 'ALIYUN', 'BAIDU_PRO', 'GOOGLE', 'MICROSOFT', 'SOGOU', 'TENCENT', 'YITU'], default='ALIYUN', help='speech recognition engine.')
  parser.add_argument('--id', type=str, default='', help='a user defined string, used as session id, if not specified, uuid.uuid1() will be used.')
  parser.add_argument('--audio', type=str, default='', help='audio file, if not specified, stdin will be used.')
  args = parser.parse_args()

  session_id = args.id if args.id else uuid.uuid1()

  ## assume audio has been converted to 16k16bit pcm/wav
  #audio = open(args.audio, mode = 'rb') if args.audio else sys.stdin.buffer
  #audio_data = audio.read()

  # use ffmpeg to convert input audio stream to desired format(16k16bit mono wav) on-the-fly
  audio_bytes = read_audio_and_convert_to_bytes(args.audio if args.audio else '-')

  # its not ideal to dump audio bytes to tmp file and load it back,
  # this is because torchaudio's "file-like loading" is not released yet
  # see https://github.com/pytorch/audio/pull/1158
  with open('tmp.wav', 'wb') as f:
    f.write(audio_bytes)
  audio_tensor = read_audio_and_convert_to_tensor('tmp.wav')

  vad_model = load_jit_vad_model(os.path.join('third_party', 'silero_vad', 'files', 'model.jit'))
  segments = get_speech_segments(vad_model, audio_tensor, SITO_SAMPLE_RATE, smooth_length=2)

  for k,s in enumerate(segments):
    b = FFMPEG_WAV_HEADER_BYTES + s[0] * FFMPEG_WAV_SAMPLE_BYTES
    e = FFMPEG_WAV_HEADER_BYTES + s[1] * FFMPEG_WAV_SAMPLE_BYTES
    text = sito(args.service, F'{session_id}_{k:04d}', audio_bytes[b:e])
    print(F"<bos:{s[0]/SITO_SAMPLE_RATE}> {text} <eos:{s[1]/SITO_SAMPLE_RATE}>", file=sys.stdout)
